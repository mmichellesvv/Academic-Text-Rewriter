{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e8b72b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --token TOKEN input_file output_file\n",
      "ipykernel_launcher.py: error: the following arguments are required: output_file, --token\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz\n",
    "from docx import Document\n",
    "import requests\n",
    "from typing import List, Tuple\n",
    "from langdetect import detect\n",
    "import time\n",
    "from docx.shared import RGBColor, Pt\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "import argparse\n",
    "\n",
    "# === 0. CLI Argument Parser ===\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"**Controlled** way of rewriting academic text from PDF or DOCX.\")\n",
    "    parser.add_argument(\"input_file\", type=str, help=\"Path to input file (.pdf or .docx)\")\n",
    "    parser.add_argument(\"output_file\", type=str, help=\"Path to output file (.docx)\")\n",
    "    parser.add_argument(\"--token\", type=str, required=True, help=\"GROQ API token\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "# === 1. Extract text from PDF pages/chunks ===\n",
    "\n",
    "def extract_full_text_by_chunks_from_pdf(pdf_path: str, chunk_size: int = 12) -> List[str]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    chunks = []\n",
    "\n",
    "    for page in doc:\n",
    "        page_text = page.get_text().strip()\n",
    "        if len(page_text) < 40:\n",
    "            continue\n",
    "\n",
    "        # –†–æ–∑–±–∏–≤–∞—î–º–æ —Ç–µ–∫—Å—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–∞ ¬´–ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏¬ª –ø–æ –¥–≤–æ–∫—Ä–∞—Ç–Ω–∏—Ö –ø–µ—Ä–µ–Ω–æ—Å–∞—Ö —Ä—è–¥–∫–∞\n",
    "        paragraphs = [p.strip() for p in page_text.split(\"\\n\\n\") if p.strip()]\n",
    "\n",
    "        i = 0\n",
    "        carry_over = None\n",
    "\n",
    "        while i < len(paragraphs):\n",
    "            current_chunk = paragraphs[i:i + chunk_size]\n",
    "\n",
    "            if carry_over:\n",
    "                current_chunk = [carry_over] + current_chunk\n",
    "                carry_over = None\n",
    "\n",
    "            # –Ø–∫—â–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–∞ \":\", –ø–µ—Ä–µ–Ω–æ—Å–∏–º–æ –π–æ–≥–æ –≤ –Ω–∞—Å—Ç—É–ø–Ω–∏–π chunk\n",
    "            if current_chunk and current_chunk[-1].endswith(\":\"):\n",
    "                carry_over = current_chunk.pop(-1)\n",
    "\n",
    "            chunk_text = \"\\n\\n\".join(current_chunk)\n",
    "            if chunk_text.strip():\n",
    "                chunks.append(chunk_text)\n",
    "\n",
    "            i += chunk_size\n",
    "\n",
    "        if carry_over:\n",
    "            chunks.append(carry_over)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# === 2. Extract text from DOCX pages/chunks ===\n",
    "def extract_full_text_by_chunks_from_docx(docx_path: str, chunk_size: int = 12) -> List[str]:\n",
    "    doc = Document(docx_path)\n",
    "    paragraphs = [p.text.strip() for p in doc.paragraphs if len(p.text.strip()) > 20]\n",
    "\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    carry_over = None\n",
    "\n",
    "    while i < len(paragraphs):\n",
    "        current_chunk = paragraphs[i:i + chunk_size]\n",
    "\n",
    "        if carry_over:\n",
    "            current_chunk = [carry_over] + current_chunk\n",
    "            carry_over = None\n",
    "\n",
    "        if current_chunk and current_chunk[-1].endswith(\":\"):\n",
    "            carry_over = current_chunk.pop(-1)\n",
    "\n",
    "        chunk_text = \"\\n\\n\".join(current_chunk)\n",
    "        if chunk_text.strip():\n",
    "            chunks.append(chunk_text)\n",
    "\n",
    "        i += chunk_size\n",
    "\n",
    "    if carry_over:\n",
    "        chunks.append(carry_over)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# === 3. Paraphrase using Groq API ===\n",
    "def paraphrase_groq(text: str, token: str) -> str:\n",
    "    try:\n",
    "        tx_lang = detect(text)\n",
    "    except Exception as e:\n",
    "        print(f\"! Cannot detect the language of the written text: {e}\")\n",
    "        tx_lang = \"en\"  # fallback\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"llama3-70b-8192\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    f\"–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç —É —Å—Ñ–µ—Ä—ñ. –ó–∞—Å—Ç–æ—Å—É–π –Ω–∞—Å—Ç—É–ø–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–æ **–∫–æ–∂–Ω–æ–≥–æ** –∞–±–∑–∞—Ü—É: \"\n",
    "                    f\"–ü–µ—Ä–µ–±—É–¥—É–π —Ä–µ—á–µ–Ω–Ω—è –∞–±–æ —ó—Ö —á–∞—Å—Ç–∏–Ω–∏ ‚Äî —Ç–∞–∫, —â–æ–± –ø–æ—Ä—è–¥–æ–∫ —Å–ª—ñ–≤ –±—É–≤ —ñ–Ω—à–∏–º, –∞–ª–µ —Å–µ–Ω—Å –∑–∞–ª–∏—à–∏–≤—Å—è —Ç–∏–º —Å–∞–º–∏–º. \"\n",
    "                    f\"–ü–µ—Ä–µ–ø–∏—à–∏ —Ç–µ–∫—Å—Ç —ñ–Ω—à–∏–º–∏ —Å–ª–æ–≤–∞–º–∏, –∑–º—ñ–Ω—é—é—á–∏ –ª–∏—à–µ 10‚Äì20% —Ñ–æ—Ä–º—É–ª—é–≤–∞–Ω—å, –∞–ª–µ –ø–æ–≤–Ω—ñ—Å—Ç—é –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ –∑–º—ñ—Å—Ç. \"\n",
    "                    f\"–î–æ–¥–∞–π 5% –Ω–æ–≤–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –∞–±–æ –¥–µ—Ç–∞–ª–µ–π, —â–æ –∑–±–∞–≥–∞—á—É—é—Ç—å —Ç–µ–∫—Å—Ç. \"\n",
    "                    f\"–ó–±–µ—Ä–µ–∂–∏ –∞–∫–∞–¥–µ–º—ñ—á–Ω–æ-–æ—Ñ—ñ—Ü—ñ–π–Ω–∏–π —Å—Ç–∏–ª—å. –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ {tx_lang} –º–æ–≤–æ—é. \"\n",
    "                    f\"–ü–æ–≤–µ—Ä–Ω–∏ –ª–∏—à–µ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç –±–µ–∑ –≤—Å—Ç—É–ø—É –∞–±–æ –ø–æ—è—Å–Ω–µ–Ω—å.\"\n",
    "                )\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"frequency_penalty\": 0.4,\n",
    "        \"presence_penalty\": 0.2\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\"https://api.groq.com/openai/v1/chat/completions\", headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"! Paraphrasing error: {e}\")\n",
    "        return \"[Paraphrasing Error]\"\n",
    "\n",
    "# === 4. Save to DOCX ===\n",
    "def write_to_docx_by_page(pages: List[Tuple[str, str]], output_path: str):\n",
    "    doc = Document()\n",
    "    doc.add_heading(\"Academic Rewriting\", 0)\n",
    "\n",
    "    for idx, (original, paraphrased) in enumerate(pages, 1):\n",
    "        heading = doc.add_heading(f\"P{idx}\\n\", level=1)\n",
    "\n",
    "        run = heading.runs[0]\n",
    "        font = run.font\n",
    "        font.name = 'Arial'           \n",
    "        font.size = Pt(16)            \n",
    "        font.bold = True             \n",
    "        font.color.rgb = RGBColor(0x00, 0x33, 0x66)  \n",
    "        heading.alignment = WD_ALIGN_PARAGRAPH.LEFT \n",
    "\n",
    "        # –î–æ–¥–∞—î–º–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∑ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–æ–≤–∞–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º\n",
    "        doc.add_paragraph(paraphrased)\n",
    "        doc.add_paragraph(\"\")\n",
    "\n",
    "    doc.save(output_path)\n",
    "\n",
    "# === 5. Main Process ===\n",
    "def process_document(input_path: str, output_path: str, token: str):\n",
    "    ext = os.path.splitext(input_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        pages = extract_full_text_by_chunks_from_pdf(input_path)\n",
    "    elif ext == \".docx\":\n",
    "        pages = extract_full_text_by_chunks_from_docx(input_path)\n",
    "    else:\n",
    "        raise ValueError(\"Only .pdf and .docx formats are supported!\")\n",
    "\n",
    "    print(f\"\\n- Processing {len(pages)} pages/chunks...\")\n",
    "    final_result = []\n",
    "\n",
    "    for i, text in enumerate(pages):\n",
    "        print(f\"\\nüìÑ Page {i + 1}\")\n",
    "        paraphrased = paraphrase_groq(text, token)\n",
    "        final_result.append((text, paraphrased))\n",
    "        time.sleep(0.7)\n",
    "\n",
    "    write_to_docx_by_page(final_result, output_path)\n",
    "    print(f\"\\n- Done successfuly! Head to: {output_path}\")\n",
    "\n",
    "# === 6. Entry Point ===\n",
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    process_document(args.input_file, args.output_file, args.token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba7186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
